# data-analysis

## Graph
- 直方图(histogram)
    >适用于描述连续数据的集中趋势和离散程度,主要用来展示数据的分布情况,每个柱子表示一个区间内的数据频率

- 柱状图(bar)
    >适用于展示分类数据的计数、频率或其他度量,主要用来做差异比较，不同类别之间的数值大小

- 饼图(pie)
    >表示占比，要注意变种有很多，可以是多环图，嵌套饼图（表示大类占比，大类中的各个子类占比等等）等。也可以使用饼图和直方图结合。

- 热力图(heatmap)
    >通过颜色变化展示数据密度、分布及变化趋势的可视化工具，有很高的空间利用率，色块可以紧凑排列，颜色的深度
    可以跨越较大数据范围，并且人类本身对颜色的理解要由于对数字的理解

- 箱线图(boxplot)
    >通过数据四分位数形成的一种可视化数据偏态尾重分布，离群点的方法。主要价值在于对异常值有耐抗性，箱体集中表示了50%的数据范围，异常值以点的形式表示
    箱线图的理解首先要理解四分位数。
    箱体（Box）：箱体的底部表示 Q1。箱体的顶部表示 Q3。箱体的中间线表示中位数（Q2）。
    须（Whiskers）：须表示数据的上下边界，通常延伸到 Q1−1.5×IQR 和 Q3+1.5×IQR 的最远数据点。
    异常值（Outliers）：须之外的数据点被认为是异常值，通常用点（dots）表示。

- 小提琴图(violin plot)
    >一种用于展示数据分布及其概率密度的图形。这种图形在箱线图的基础上叠加显示了数据的概率密度，形状类似小提琴，因此得名。
    该图非常适合用于比较多个组或类别之间的数据分布。通过并排展示多个小提琴图，可以直观地比较不同组之间的数据分布差异，识别其中的相似性和差异性。例如，在基因表达分析中，小提琴图可以用来比较不同实验组之间的基因表达水平。

- 散点矩阵图(pair plot / scatter matrix plot)
    >是一种用于展示多个变量之间关系的可视化工具。它通过在一个矩阵布局中展示多个变量对的散点图，帮助快速识别变量间的相关性,适用于多变量关系分析，数据探索性分析，异常值检测，市场调研金融等领域的群体比较等方面。

- 六边形分箱图(Hexagonal Binning)
    >一种用于可视化大量散点数据的图形工具，适用于在二维空间中显示数据点的密度。它结合了散点图和直方图的特点，通过将数据点聚合到六边形单元格中，并用颜色深浅表示每个单元格内的数据点数量，从而有效地展示数据的分布情况。具有以下价值：
    1. 高效处理高密度数据：在面对大量数据点时，传统的散点图可能会因为数据点的重叠而难以解释，而 Hexbin 图能够有效地解决这一问题，使数据分布的模式更加清晰可见。
    2. 空间利用率高：六边形的形状使得它们在平面上比矩形更紧密地排列，减少了空隙，从而提高了数据的空间利用率。
    3. 平滑的密度估计：六边形的形状使得相邻的单元之间的连接更加自然，有助于在视觉上呈现数据的分布趋势。
    4. 适应性强：可以处理大量数据点，尤其是在数据点数量较多时，Hexbin 图能够有效地展示数据的整体分布，而不会因为数据点的重叠而导致信息丢失


<!--->
数学分支及对应书籍

离散函数，差分，二阶差分和 Laplacian 滤波原理
拉普拉斯算子
拉普拉斯矩阵

<--->

## Terms

- 概率密度/概率
    >概率密度函数（Probability Density Function, PDF）的值本身不是概率，而是概率的密度。这意味着在连续随机变量的情况下，PDF 的值表示在某个特定值附近的单位长度内的概率。
    概率P和概率密度f(x)的关系就是: 
    $$P(a \leq X \leq b) = \int_a^bf(x)\,dx$$

- 核密度估计（Kernel Density Estimation, KDE）
    >由给定样本集合求解随机变量的分布密度函数问题是概率统计学的基本问题之一。
    解决这一问题的方法包括参数估计和非参数估计。
    >- 参数估计：
    参数估计方法中，分为参数回归分析和参数判别分析。
    >   1. 参数回归分析：人们假定数据分布符合某种特定的性态，如线性、可化线性或指数性态等，然后在目标函数族中寻找特定的解，即确定回归模型中的未知参数，适用于因变量为连续变量的情况，如预测房价、销售额等
    >   2. 参数判别分析：用于根据一组特征将数据分配到不同的类别中。它假设每个类别的数据来自多元正态分布，并通过估计每个类别的均值和协方差矩阵来建立判别规则，适用于因变量为离散类别的情况，如分类鸢尾花种类、识别手写数字等
    >- 非参数估计：参数模型的这种基本假定与实际的物理模型之间常常存在较大的差距，这些方法并非总能取得令人满意的结果，Rosenblatt和Parzen提出了非参数估计方法，即核密度估计方法。由于核密度估计方法不利用有关数据分布的先验知识，对数据分布不附加任何假定，是一种从数据样本本身出发研究数据分布特征的方法
    >- KDE就是在概率论中用来估计未知的密度函数，属于非参数检验方法之一。  
    核密度函数的原理比较简单，在我们知道某一事物的概率分布的情况下，如果某一个数在观察中出现了，我们可以认为这个数的概率密度很大，和这个数比较近的数的概率密度也会比较大，而那些离这个数远的数的概率密度会比较小。  
    实现上是采用平滑的峰值函数(“核”)来拟合观察到的数据点，从而对真实的概率分布曲线进行模拟。   
    >   - 核密度估计的公式为：   
        $$\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^{n} K(\frac{x - x_i}{h})$$   
    >   $\hat{f}(x)$  :是估计的概率密度函数  
    >   $n$    :是数据点的数量  
    >   $x_i$  :是第i个随机数据点  
    >   $h$    :是带宽，也叫窗口,控制核函数的宽度  
    >   $K$    :是核函数，通常是一个非负的、对称的函数，有高斯核、均匀核等函数  
    >   注意!!! 这里的核函数概念和低维映射高维用到的核函数不是一个概念！！  

- 核函数(kernel function)
    >“你在你的一生中可能会经历很多变故，可能会变成完全不同的另一个人，但是这个世界上只有一个你，我要怎样才能把不同的“你”分开呢？最直观的方法就是增加“时间”这个维度，虽然这个地球上只有一个你，这个你是不可分割的，但是“昨天在中国的你”和“今天在美国的你”在时间+空间这个维度却是可以被分割的。”
    可以通过增加维度，解决在原始空间上无法线性可分的问题，如果原始空间是有限维的，那么一定存在一个高维特征空间使样本可分
    kernel其实就是帮我们省去在高维空间里进行繁琐计算的“简便运算法”。甚至，它能解决无限维空间无法计算的问题

- Mercer定理
    >指任何半正定的函数都可以作为核函数。具体来说，Mercer定理表明，如果一个函数是半正定的（即对于任何输入向量，该函数值为非负），那么它可以被视为一个核函数。这意味着这个函数可以用来计算两个向量在某个高维空间中的内积，而不需要显式地将数据映射到这个高维空间中

- 向量内积(Dot Product) / 外积(Cross Product)
    >内积本质衡量两个向量在方向上的相似程度。它可以通过向量的模长和它们之间的夹角来表示
    >外积本质是生成一个与两个原始向量都垂直的向量，其模长等于两个原始向量组成的平行四边形的面积。外积的方向遵循右手定则

- 方差优良数学性质的理解
    >这里的优良主要指的是方便计算和操作。
    方差：具有优良的数学性质，如可加性、线性变换的可预测性、非负性等，适合进行进一步的统计分析。
    标准差：量纲上和原始数据集一致，便于理解

- 离群数据 (Outliers)
    >在数据集中明显偏离其他数据点的值。这些数据点可能由于测量误差、数据录入错误、自然变异或其他异常情况而显著不同于其他数据,有以下类型：
    >1. 全局离群数据（Global Outliers）：在整个数据集中显得异常的数据点。
    >2. 上下文离群数据（Contextual Outliers）：在特定上下文中显得异常的数据点，但在其他上下文中可能不异常。
    >3. 集体离群数据（Collective Outliers）：一组数据点在整体上显得异常，但单个数据点可能不异常。

- 四分位数 ( Quartiles )
    >数据的四分位数（Quartiles）是具体的数值，而不是一个范围。四分位数将数据集分为四个部分，每个部分包含25%的数据点
    第一四分位数（Q1）：数据集中25%的数据小于或等于这个值。
    第二四分位数（Q2）：即中位数，数据集中50%的数据小于或等于这个值。
    第三四分位数（Q3）：数据集中75%的数据小于或等于这个值。
    四分位距（IQR）：表示数据中间50%的范围，计算公式为 IQR=Q3−Q1;
    异常值范围：通常定义为小于 Q1−1.5×IQR 或大于 Q3+1.5×IQR 的值
    四分位数有以下作用：
    >1. 描述数据分布：通过 Q1、Q2 和 Q3，可以了解数据的集中趋势和离散程度。
    >2. 检测异常值：通过 IQR，可以识别数据中的异常值。
    >3. 比较不同数据集：通过四分位数，可以比较不同数据集的分布情况。
    四分位数的计算方法主要依赖插值法，这个思想可以借鉴，一种通过已知点的值以及值间的距离来估算未知值的方法
    >1. 找到位置 p 的整数部分 ⌊p⌋ 和向上取整部分 ⌈p⌉。
    >2. 使用这两个位置的值进行线性插值，得到四分位数的估计值。(四分位数值=位置⌊p⌋的值+(p−⌊p⌋)×(位置⌈p⌉的值−位置⌊p⌋的值))

- 虚拟变量(也称为独热编码，One-Hot Encoding)
    >一种将分类变量转换为数值变量的方法。每个分类变量的类别(category)会被转换为一个独立的列，列中只有有 0 和 1 两种值，表示该类别是否出现

- 多重共线性(Multicollinearity)
    >统计学和机器学习中一个重要的概念，指的是在回归模型中，两个或多个解释变量(自变量)之间存在高度相关性。这种相关性可能导致模型的估计结果不稳定，参数解释不准确，甚至影响模型的预测性能。

- VIF
    >检测多重共线性的常用指标。VIF 值越高，表示该变量与其他变量之间的共线性越强。



## Reference
- [matplotlib官方文档及demo](https://matplotlib.org/stable/gallery/pie_and_polar_charts/bar_of_pie.html)
- [sql join](https://learnsql.com/blog/sql-join-cheat-sheet/joins-cheat-sheet-a4.pdf)
- [统计学讲义](https://www.math.pku.edu.cn/teachers/lidf/course/probstathsy/probstathsy.pdf)
- [箱线图介绍](https://www.stat.cmu.edu/~hseltman/309/Book/chapter4.pdf)
- [核函数](https://geekdaxue.co/read/myheros@tbcnzy/gl6ubi#eimgx7)
  



